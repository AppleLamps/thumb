---
url: "https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model"
title: "Deploy a Text-to-Speech Model - fal"
---

[Skip to main content](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#content-area)

[fal home page![light logo](https://mintcdn.com/fal-d8505a2e/_1QeqsRe91WUAOCJ/logo/light.svg?fit=max&auto=format&n=_1QeqsRe91WUAOCJ&q=85&s=04c374284984bf56c89974379f02b7a2)![dark logo](https://mintcdn.com/fal-d8505a2e/_1QeqsRe91WUAOCJ/logo/dark.svg?fit=max&auto=format&n=_1QeqsRe91WUAOCJ&q=85&s=b136c77964ac416a72cb0bcba775d7c7)](https://fal.ai/)

Search...

Ctrl KAsk AI

Search...

Navigation

Tutorials

Deploy a Text-to-Speech Model

[![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/home.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/home.svg)Home](https://docs.fal.ai/) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/ar-cube-1.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/ar-cube-1.svg)Model APIs](https://docs.fal.ai/model-apis) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/rocket.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/rocket.svg)Serverless](https://docs.fal.ai/serverless) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/chip.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/chip.svg)Compute](https://docs.fal.ai/compute) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/file-json.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/file-json.svg)Platform APIs](https://docs.fal.ai/platform-apis) [Changelog](https://docs.fal.ai/changelog)

- [Status](https://status.fal.ai/)
- [Community](https://discord.gg/fal-ai)
- [Blog](https://blog.fal.ai/)

- [Introduction](https://docs.fal.ai/serverless)

- [Connect to Cursor](https://docs.fal.ai/serverless/mcp)

##### Getting Started

- [Quick Start](https://docs.fal.ai/serverless/getting-started/quick-start)
- [Deploy Your First Image Generator](https://docs.fal.ai/serverless/getting-started/deploy-your-first-image-generator)
- [Installation & Setup](https://docs.fal.ai/serverless/getting-started/installation)
- [Core Concepts](https://docs.fal.ai/serverless/getting-started/core-concepts)

##### Tutorials

- [Deploy a Text-to-Image Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-image-model)
- [Deploy a Text-to-Video Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-video-model)
- [Deploy a Text-to-Speech Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model)
- [Deploy a Text-to-Music Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-music-model)
- [Deploy a ComfyUI SDXL Turbo App](https://docs.fal.ai/serverless/tutorials/deploy-comfyui-server)
- [Deploy Multi-GPU Inference](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-inference)
- [Deploy Models with Custom Containers](https://docs.fal.ai/serverless/tutorials/deploy-models-with-custom-containers)

##### Deployment & Operations

- [Deploy to Production](https://docs.fal.ai/serverless/deployment-operations/deploy-to-production)
- [Manage Deployments](https://docs.fal.ai/serverless/deployment-operations/manage-deployments)
- [Manage Secrets Securely](https://docs.fal.ai/serverless/deployment-operations/manage-secrets-securely)
- [Monitor Performance](https://docs.fal.ai/serverless/deployment-operations/monitor-performance)
- [Scale Your Application](https://docs.fal.ai/serverless/deployment-operations/scale-your-application)

##### Development

- [Handle Inputs and Outputs](https://docs.fal.ai/serverless/development/handle-inputs-and-outputs)
- [Download Model Weights and Files](https://docs.fal.ai/serverless/development/download-model-weights-and-files)
- [Import Code](https://docs.fal.ai/serverless/development/import-code)
- [Use Persistent Storage](https://docs.fal.ai/serverless/development/use-persistent-storage)
- [Streaming Endpoints](https://docs.fal.ai/serverless/development/streaming)
- [Realtime Endpoints](https://docs.fal.ai/serverless/development/realtime)
- [Test Models and Endpoints](https://docs.fal.ai/serverless/development/test-models-and-endpoints)
- [Use a Custom Container Image](https://docs.fal.ai/serverless/development/use-custom-container-image)
- [Handle request cancellations](https://docs.fal.ai/serverless/development/handle-cancellations)
- [Use KV Store](https://docs.fal.ai/serverless/development/use-kv-store)
- [Add Health Check Endpoint](https://docs.fal.ai/serverless/development/add-health-check-endpoint)

##### Multi-GPU Workloads

- [Overview](https://docs.fal.ai/serverless/distributed/overview)
- [Event Streaming](https://docs.fal.ai/serverless/distributed/streaming)
- [API Reference](https://docs.fal.ai/serverless/distributed/api-reference)

##### Advanced Optimizations

- [Optimize Routing Behavior](https://docs.fal.ai/serverless/optimizations/optimize-routing-behavior)
- [Optimize Model Performance](https://docs.fal.ai/serverless/optimizations/optimize-model-performance)
- [Optimize Startup with Compiled Caches](https://docs.fal.ai/serverless/optimizations/optimize-startup-with-compiled-caches)
- [Optimize Container Images](https://docs.fal.ai/serverless/optimizations/optimize-container-images)

##### Migrations

- [Migrate an External Docker Server](https://docs.fal.ai/serverless/migrations/migrate-external-docker-server)
- [Migrate from Replicate](https://docs.fal.ai/serverless/migrations/migrate-from-replicate)

##### CLI Reference

- [Installation](https://docs.fal.ai/serverless/cli/installation)
- [fal auth](https://docs.fal.ai/serverless/cli/auth)
- fal apps

- [fal deploy](https://docs.fal.ai/serverless/cli/deploy)
- [fal files](https://docs.fal.ai/serverless/cli/files)
- [fal run](https://docs.fal.ai/serverless/cli/run)
- [fal queue](https://docs.fal.ai/serverless/cli/queue)
- [fal keys](https://docs.fal.ai/serverless/cli/keys)
- [fal profile](https://docs.fal.ai/serverless/cli/profile)
- [fal secrets](https://docs.fal.ai/serverless/cli/secrets)
- [fal doctor](https://docs.fal.ai/serverless/cli/doctor)
- [fal create](https://docs.fal.ai/serverless/cli/create)
- [fal runners](https://docs.fal.ai/serverless/cli/runners)

##### Python SDK

- [fal.App Class Reference](https://docs.fal.ai/serverless/python/fal-app-reference)
- [fal.api.SyncServerlessClient](https://docs.fal.ai/serverless/python/client)
- [Python SDK API Reference](https://docs.fal.ai/serverless/python/api-reference)

##### API Reference

- [Platform APIs for Serverless](https://docs.fal.ai/serverless/platform-apis)

On this page

- [üöÄ Try this Example](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#%F0%9F%9A%80-try-this-example)
- [Key Features](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#key-features)
- [When to Use CPU Deployment](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#when-to-use-cpu-deployment)
- [Project Setup](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#project-setup)
- [Language-Specific Input Models](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#language-specific-input-models)
- [Language-Specific Output Models](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#language-specific-output-models)
- [Application Configuration for CPU Deployment](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#application-configuration-for-cpu-deployment)
- [Shared Generation Logic](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#shared-generation-logic)
- [Multiple Endpoint Definitions](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#multiple-endpoint-definitions)
- [Key Concepts and Best Practices](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#key-concepts-and-best-practices)
- [CPU-Efficient Deployment](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#cpu-efficient-deployment)
- [Audio Streaming and Memory Management](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#audio-streaming-and-memory-management)
- [Character-Based Billing](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#character-based-billing)
- [Audio File Handling](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#audio-file-handling)
- [Multi-Language Architecture](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#multi-language-architecture)
- [Advanced Features](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#advanced-features)
- [Custom Validation](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#custom-validation)
- [Backwards Compatibility](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#backwards-compatibility)
- [Flexible Text Processing](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#flexible-text-processing)
- [Deployment and Usage](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#deployment-and-usage)
- [Running the Service](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#running-the-service)
- [Making Requests](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#making-requests)
- [Use Cases](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#use-cases)
- [Performance Optimizations](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#performance-optimizations)
- [Memory Efficiency](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#memory-efficiency)
- [Cost Optimization](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#cost-optimization)
- [Key Takeaways](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model#key-takeaways)

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#%F0%9F%9A%80-try-this-example)  üöÄ Try this Example

View the complete source code on [GitHub](https://github.com/fal-ai-community/fal-demos/blob/main/fal_demos/tts/kokoro.py).**Steps to run:**

1. Install fal:

Report incorrect code

Copy

Ask AI

```
pip install fal
```

2. Authenticate (if not already done):

Report incorrect code

Copy

Ask AI

```
fal auth login
```

3. Copy the code below into `kokoro.py`

kokoro.py

pyproject.toml

Report incorrect code

Copy

Ask AI

```
from typing import Literal

import fal
from fal.exceptions import FieldException
from fal.toolkit import File
from fastapi import Response
from pydantic import BaseModel, Field

class AmEnglishRequest(BaseModel):
    prompt: str = Field(
        default="",
        examples=[\
            "The future belongs to those who believe in the beauty of their dreams. So, dream big, work hard, and make it happen!"\
        ],
        ui={"important": True},
    )
    text: str = Field(
        default="",
        examples=[\
            "The future belongs to those who believe in the beauty of their dreams. So, dream big, work hard, and make it happen!"\
        ],
    )
    # Use Literal for voice to restrict to specific enum values
    voice: Literal[\
        "af_heart",\
        "af_alloy",\
        "af_aoede",\
        "af_bella",\
        "af_jessica",\
        "af_kore",\
        "af_nicole",\
        "af_nova",\
        "af_river",\
        "af_sarah",\
        "af_sky",\
        "am_adam",\
        "am_echo",\
        "am_eric",\
        "am_fenrir",\
        "am_liam",\
        "am_michael",\
        "am_onyx",\
        "am_puck",\
        "am_santa",\
    ] = Field(
        examples=["af_heart"],
        default="af_heart",
        description="Voice ID for the desired voice.",
    )
    speed: float = Field(
        default=1.0,
        ge=0.1,
        le=5.0,
        description="Speed of the generated audio. Default is 1.0.",
    )

class BrEnglishRequest(BaseModel):
    prompt: str = Field(
        examples=[\
            "Ladies and gentlemen, welcome aboard. Please ensure your seatbelt is fastened and your tray table is stowed as we prepare for takeoff."\
        ]
    )
    voice: Literal[\
        "bf_alice",\
        "bf_emma",\
        "bf_isabella",\
        "bf_lily",\
        "bm_daniel",\
        "bm_fable",\
        "bm_george",\
        "bm_lewis",\
    ] = Field(
        examples=["bf_alice"],
        description="Voice ID for the desired voice.",
    )
    speed: float = Field(
        default=1.0,
        ge=0.1,
        le=5.0,
        description="Speed of the generated audio. Default is 1.0.",
    )

class JapaneseRequest(BaseModel):
    prompt: str = Field(examples=["Â§¢„ÇíËøΩ„ÅÑ„Åã„Åë„Çã„Åì„Å®„ÇíÊÅê„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇÂä™Âäõ„Åô„Çå„Å∞„ÄÅÂøÖ„ÅöÈÅì„ÅØÈñã„Åë„Åæ„ÅôÔºÅ"])
    voice: Literal[\
        "jf_alpha",\
        "jf_gongitsune",\
        "jf_nezumi",\
        "jf_tebukuro",\
        "jm_kumo",\
    ] = Field(
        examples=["jf_alpha"],
        description="Voice ID for the desired voice.",
    )
    speed: float = Field(
        default=1.0,
        ge=0.1,
        le=5.0,
        description="Speed of the generated audio. Default is 1.0.",
    )

class AmEngOutput(BaseModel):
    audio: File = Field(
        description="The generated music",
        examples=[\
            File._from_url(\
                "https://fal.media/files/elephant/dXVMqWsBDG9yan3kaOT0Z_tmp0vvkha3s.wav"\
            )\
        ],
    )

class BrEngOutput(BaseModel):
    audio: File = Field(
        description="The generated music",
        examples=[\
            File._from_url(\
                "https://fal.media/files/kangaroo/4wpA60Kum6UjOVBKJoNyL_tmpxfrkn95k.wav"\
            )\
        ],
    )

class JapaneseOutput(BaseModel):
    audio: File = Field(
        description="The generated music",
        examples=[\
            File._from_url(\
                "https://fal.media/files/lion/piLhqKO8LJxrWaNg2dVUv_tmpp6eff6zl.wav"\
            )\
        ],
    )

class Kokoro(fal.App):
    min_concurrency = 0
    max_concurrency = 1
    keep_alive = 3000
    app_name = "kokoro"
    requirements = [\
        "kokoro==0.8.4",\
        "soundfile==0.13.1",\
        "misaki[en]==0.8.4",\
        "misaki[ja]==0.8.4",\
        "misaki[zh]==0.8.4",\
        "numpy==1.26.4",\
        "https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl",\
    ]
    machine_type = "L"  # Use a CPU machine type since Kokoro is only 82M parameters and runs efficiently on CPU

    async def setup(self):
        from kokoro import KPipeline

        self.pipelines = {}
        self.pipelines["American English"] = KPipeline(lang_code="a")
        self.pipelines["British English"] = KPipeline(lang_code="b")
        self.pipelines["Japanese"] = KPipeline(lang_code="j")

    async def _generate(
        self,
        request: AmEnglishRequest,
        response: Response,
        language: str = "American English",
    ):
        prompt = request.prompt or request.text
        if len(prompt) >= 20000:
            # Use fieldexception to nicely render the error in the UI
            raise FieldException(
                field="prompt",
                message="Prompt must be less than 20000 characters.",
            )

        import tempfile

        import numpy as np
        import soundfile as sf

        pipeline = self.pipelines[language]
        generator = pipeline(
            prompt,
            voice=request.voice,
            speed=request.speed,
            split_pattern=r"\n+",
        )
        for i, (gs, ps, audio) in enumerate(generator):
            if i == 0:
                final_audio = audio.detach().cpu().numpy()
            else:
                audio = audio.detach().cpu().numpy()
                final_audio = np.concatenate((final_audio, audio), axis=0)

        # Set the billing units to be a minimum of 1 and scale with 1000 characters
        response.headers["x-fal-billable-units"] = str(max(1, len(prompt) // 1000))

        # Use a temporary file to save the audio and then send it via the cdn through the File object
        with tempfile.NamedTemporaryFile(suffix=".wav") as f:
            sf.write(f.name, final_audio, 24000)
            return AmEngOutput(
                audio=File.from_path(f.name, content_type="audio/wav", repository="cdn")
            )

    @fal.endpoint("/")
    async def generate(
        self, request: AmEnglishRequest, response: Response
    ) -> AmEngOutput:
        return await self._generate(request, response, language="American English")

    @fal.endpoint("/american-english")
    async def generate_am_english(
        self, request: AmEnglishRequest, response: Response
    ) -> AmEngOutput:
        return await self._generate(request, response, language="American English")

    @fal.endpoint("/british-english")
    async def generate_br_english(
        self, request: BrEnglishRequest, response: Response
    ) -> BrEngOutput:
        return await self._generate(request, response, language="British English")

    @fal.endpoint("/japanese")
    async def generate_japanese(
        self, request: JapaneseRequest, response: Response
    ) -> JapaneseOutput:
        return await self._generate(request, response, language="Japanese")

...  # Define the rest of the languages as endpoints similarly
```

4. Run the app:

Report incorrect code

Copy

Ask AI

```
fal run kokoro.py
```

**Or clone this repository**:

Report incorrect code

Copy

Ask AI

```
git clone https://github.com/fal-ai-community/fal-demos.git
cd fal-demos
pip install -e .
# Use the app name (defined in pyproject.toml)
fal run kokoro
# Or use the full file path:
# fal run fal_demos/tts/kokoro.py::Kokoro
```

**Before you run**, make sure you have:

- Authenticated with fal: `fal auth login`
- Activated your virtual environment (recommended): `python -m venv venv && source venv/bin/activate` (macOS/Linux) or `venv\Scripts\activate` (Windows)

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#key-features)  Key Features

- **Multi-Language Support**: American English, British English, Japanese with native voices
- **CPU-Efficient Deployment**: Lightweight 82M parameter model runs efficiently on CPU
- **Multiple Endpoints**: Language-specific endpoints with shared generation logic
- **Voice Variety**: Multiple voice options for each supported language
- **Audio Streaming**: Generator-based audio processing for memory efficiency
- **Character-Based Billing**: Usage-based pricing tied to text length
- **Advanced Validation**: Custom error handling with user-friendly messages
- **Audio File Management**: Temporary file handling and CDN integration

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#when-to-use-cpu-deployment)  When to Use CPU Deployment

CPU deployment is ideal when:

- Models are lightweight (< 100M parameters)
- Inference is fast enough on CPU
- Cost optimization is important
- GPU resources are not required
- Multiple concurrent requests can share CPU resources efficiently

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#project-setup)  Project Setup

Report incorrect code

Copy

Ask AI

```
from typing import Literal

import fal
from fal.exceptions import FieldException
from fal.toolkit import File
from fastapi import Response
from pydantic import BaseModel, Field
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#language-specific-input-models)  Language-Specific Input Models

Define input models for each supported language with appropriate voice options:

Report incorrect code

Copy

Ask AI

```
class AmEnglishRequest(BaseModel):
    prompt: str = Field(
        default="",
        examples=[\
            "The future belongs to those who believe in the beauty of their dreams. So, dream big, work hard, and make it happen!"\
        ],
        ui={"important": True},
    )
    text: str = Field(
        default="",
        examples=[\
            "The future belongs to those who believe in the beauty of their dreams. So, dream big, work hard, and make it happen!"\
        ],
    )
    voice: Literal[\
        "af_heart",    # American Female voices\
        "af_alloy",\
        "af_aoede",\
        "af_bella",\
        "af_jessica",\
        "af_kore",\
        "af_nicole",\
        "af_nova",\
        "af_river",\
        "af_sarah",\
        "af_sky",\
        "am_adam",     # American Male voices\
        "am_echo",\
        "am_eric",\
        "am_fenrir",\
        "am_liam",\
        "am_michael",\
        "am_onyx",\
        "am_puck",\
        "am_santa",\
    ] = Field(
        examples=["af_heart"],
        default="af_heart",
        description="Voice ID for the desired voice.",
    )
    speed: float = Field(
        default=1.0,
        ge=0.1,
        le=5.0,
        description="Speed of the generated audio. Default is 1.0.",
    )

class BrEnglishRequest(BaseModel):
    prompt: str = Field(
        examples=[\
            "Ladies and gentlemen, welcome aboard. Please ensure your seatbelt is fastened and your tray table is stowed as we prepare for takeoff."\
        ]
    )
    voice: Literal[\
        "bf_alice",    # British Female voices\
        "bf_emma",\
        "bf_isabella",\
        "bf_lily",\
        "bm_daniel",   # British Male voices\
        "bm_fable",\
        "bm_george",\
        "bm_lewis",\
    ] = Field(
        examples=["bf_alice"],
        description="Voice ID for the desired voice.",
    )
    speed: float = Field(
        default=1.0,
        ge=0.1,
        le=5.0,
        description="Speed of the generated audio. Default is 1.0.",
    )

class JapaneseRequest(BaseModel):
    prompt: str = Field(
        examples=["Â§¢„ÇíËøΩ„ÅÑ„Åã„Åë„Çã„Åì„Å®„ÇíÊÅê„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇÂä™Âäõ„Åô„Çå„Å∞„ÄÅÂøÖ„ÅöÈÅì„ÅØÈñã„Åë„Åæ„ÅôÔºÅ"]
    )
    voice: Literal[\
        "jf_alpha",    # Japanese Female voices\
        "jf_gongitsune",\
        "jf_nezumi",\
        "jf_tebukuro",\
        "jm_kumo",     # Japanese Male voices\
    ] = Field(
        examples=["jf_alpha"],
        description="Voice ID for the desired voice.",
    )
    speed: float = Field(
        default=1.0,
        ge=0.1,
        le=5.0,
        description="Speed of the generated audio. Default is 1.0.",
    )
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#language-specific-output-models)  Language-Specific Output Models

Report incorrect code

Copy

Ask AI

```
class AmEngOutput(BaseModel):
    audio: File = Field(
        description="The generated audio",
        examples=[\
            File._from_url(\
                "https://fal.media/files/elephant/dXVMqWsBDG9yan3kaOT0Z_tmp0vvkha3s.wav"\
            )\
        ],
    )

class BrEngOutput(BaseModel):
    audio: File = Field(
        description="The generated audio",
        examples=[\
            File._from_url(\
                "https://fal.media/files/kangaroo/4wpA60Kum6UjOVBKJoNyL_tmpxfrkn95k.wav"\
            )\
        ],
    )

class JapaneseOutput(BaseModel):
    audio: File = Field(
        description="The generated audio",
        examples=[\
            File._from_url(\
                "https://fal.media/files/lion/piLhqKO8LJxrWaNg2dVUv_tmpp6eff6zl.wav"\
            )\
        ],
    )
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#application-configuration-for-cpu-deployment)  Application Configuration for CPU Deployment

Report incorrect code

Copy

Ask AI

```
class Kokoro(fal.App):
    min_concurrency = 0
    max_concurrency = 1
    keep_alive = 3000  # Longer keep-alive for TTS services
    app_name = "kokoro"
    requirements = [\
        "kokoro==0.8.4",\
        "soundfile==0.13.1",\
        "misaki[en]==0.8.4",  # English language support\
        "misaki[ja]==0.8.4",  # Japanese language support\
        "misaki[zh]==0.8.4",  # Chinese language support\
        "numpy==1.26.4",\
        # Spacy model for English NLP\
        "https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl",\
    ]
    machine_type = "L"  # CPU machine - efficient for lightweight models

    async def setup(self):
        from kokoro import KPipeline

        # Initialize pipelines for each supported language
        self.pipelines = {}
        self.pipelines["American English"] = KPipeline(lang_code="a")
        self.pipelines["British English"] = KPipeline(lang_code="b")
        self.pipelines["Japanese"] = KPipeline(lang_code="j")
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#shared-generation-logic)  Shared Generation Logic

Create a reusable generation method that handles all languages:

Report incorrect code

Copy

Ask AI

```
async def _generate(
    self,
    request: AmEnglishRequest,
    response: Response,
    language: str = "American English",
):
    # Handle both 'prompt' and 'text' fields for backwards compatibility
    prompt = request.prompt or request.text

    # Custom validation with user-friendly error messages
    if len(prompt) >= 20000:
        raise FieldException(
            field="prompt",
            message="Prompt must be less than 20000 characters.",
        )

    import tempfile
    import numpy as np
    import soundfile as sf

    # Get the appropriate pipeline for the language
    pipeline = self.pipelines[language]

    # Generate audio using streaming approach
    generator = pipeline(
        prompt,
        voice=request.voice,
        speed=request.speed,
        split_pattern=r"\n+",  # Split on line breaks for better pacing
    )

    # Process audio chunks and concatenate
    for i, (gs, ps, audio) in enumerate(generator):
        if i == 0:
            final_audio = audio.detach().cpu().numpy()
        else:
            audio = audio.detach().cpu().numpy()
            final_audio = np.concatenate((final_audio, audio), axis=0)

    # Character-based billing calculation
    response.headers["x-fal-billable-units"] = str(max(1, len(prompt) // 1000))

    # Save audio to temporary file and upload to CDN
    with tempfile.NamedTemporaryFile(suffix=".wav") as f:
        sf.write(f.name, final_audio, 24000)  # 24kHz sample rate
        return AmEngOutput(
            audio=File.from_path(
                f.name,
                content_type="audio/wav",
                repository="cdn"  # Upload to CDN for fast access
            )
        )
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#multiple-endpoint-definitions)  Multiple Endpoint Definitions

Define language-specific endpoints using the shared generation logic:

Report incorrect code

Copy

Ask AI

```
@fal.endpoint("/")
async def generate(
    self, request: AmEnglishRequest, response: Response
) -> AmEngOutput:
    return await self._generate(request, response, language="American English")

@fal.endpoint("/american-english")
async def generate_am_english(
    self, request: AmEnglishRequest, response: Response
) -> AmEngOutput:
    return await self._generate(request, response, language="American English")

@fal.endpoint("/british-english")
async def generate_br_english(
    self, request: BrEnglishRequest, response: Response
) -> BrEngOutput:
    return await self._generate(request, response, language="British English")

@fal.endpoint("/japanese")
async def generate_japanese(
    self, request: JapaneseRequest, response: Response
) -> JapaneseOutput:
    return await self._generate(request, response, language="Japanese")
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#key-concepts-and-best-practices)  Key Concepts and Best Practices

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#cpu-efficient-deployment)  CPU-Efficient Deployment

**Why CPU for TTS:**

- Kokoro is only 82M parameters - runs efficiently on CPU
- Lower cost compared to GPU instances
- Sufficient performance for real-time TTS
- Better resource utilization for multiple concurrent requests

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#audio-streaming-and-memory-management)  Audio Streaming and Memory Management

**Generator-based processing:**

Report incorrect code

Copy

Ask AI

```
# Stream audio generation to handle long texts efficiently
generator = pipeline(prompt, voice=request.voice, speed=request.speed)

# Process chunks incrementally
for i, (gs, ps, audio) in enumerate(generator):
    if i == 0:
        final_audio = audio.detach().cpu().numpy()
    else:
        final_audio = np.concatenate((final_audio, audio), axis=0)
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#character-based-billing)  Character-Based Billing

Report incorrect code

Copy

Ask AI

```
# Scale billing with text length (per 1000 characters)
response.headers["x-fal-billable-units"] = str(max(1, len(prompt) // 1000))
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#audio-file-handling)  Audio File Handling

Report incorrect code

Copy

Ask AI

```
# Use temporary files for audio processing
with tempfile.NamedTemporaryFile(suffix=".wav") as f:
    sf.write(f.name, final_audio, 24000)  # Save with proper sample rate
    return Output(
        audio=File.from_path(
            f.name,
            content_type="audio/wav",
            repository="cdn"  # Auto-upload to CDN
        )
    )
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#multi-language-architecture)  Multi-Language Architecture

**Pipeline initialization:**

Report incorrect code

Copy

Ask AI

```
self.pipelines = {
    "American English": KPipeline(lang_code="a"),
    "British English": KPipeline(lang_code="b"),
    "Japanese": KPipeline(lang_code="j"),
}
```

**Language-specific voice options:**

Report incorrect code

Copy

Ask AI

```
# American English voices
voice: Literal[\
    "af_heart", "af_alloy", "af_aoede",  # Female\
    "am_adam", "am_echo", "am_eric",     # Male\
]

# British English voices
voice: Literal[\
    "bf_alice", "bf_emma", "bf_lily",    # Female\
    "bm_daniel", "bm_george", "bm_lewis" # Male\
]
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#advanced-features)  Advanced Features

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#custom-validation)  Custom Validation

Report incorrect code

Copy

Ask AI

```
if len(prompt) >= 20000:
    raise FieldException(
        field="prompt",
        message="Prompt must be less than 20000 characters.",
    )
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#backwards-compatibility)  Backwards Compatibility

Report incorrect code

Copy

Ask AI

```
# Support both 'prompt' and 'text' field names
prompt = request.prompt or request.text
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#flexible-text-processing)  Flexible Text Processing

Report incorrect code

Copy

Ask AI

```
generator = pipeline(
    prompt,
    voice=request.voice,
    speed=request.speed,
    split_pattern=r"\n+",  # Split on paragraphs for natural pacing
)
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#deployment-and-usage)  Deployment and Usage

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#running-the-service)  Running the Service

Report incorrect code

Copy

Ask AI

```
# Development
fal run fal_demos/tts/kokoro.py::Kokoro

# Production deployment
fal deploy kokoro
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#making-requests)  Making Requests

**American English:**

Report incorrect code

Copy

Ask AI

```
import fal_client

result = await fal_client.submit_async(
    "your-username/kokoro/american-english",
    arguments={
        "prompt": "Hello, this is a test of American English text-to-speech!",
        "voice": "af_heart",
        "speed": 1.2
    }
)
```

**British English:**

Report incorrect code

Copy

Ask AI

```
result = await fal_client.submit_async(
    "your-username/kokoro/british-english",
    arguments={
        "prompt": "Cheerio! This is British English text-to-speech.",
        "voice": "bf_alice",
        "speed": 1.0
    }
)
```

**Japanese:**

Report incorrect code

Copy

Ask AI

```
result = await fal_client.submit_async(
    "your-username/kokoro/japanese",
    arguments={
        "prompt": "„Åì„Çì„Å´„Å°„ÅØ„ÄÅ„Åì„Çå„ÅØÊó•Êú¨Ë™û„ÅÆÈü≥Â£∞ÂêàÊàê„Åß„Åô„ÄÇ",
        "voice": "jf_alpha",
        "speed": 0.9
    }
)
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#use-cases)  Use Cases

- **Content Creation**: Generate voiceovers for videos and podcasts
- **Accessibility**: Convert text content to audio for visually impaired users
- **E-Learning**: Create educational content with natural-sounding narration
- **Customer Service**: Generate dynamic audio responses for chatbots
- **Multilingual Applications**: Support global audiences with native-sounding voices
- **Book Reading**: Convert written content to audiobooks

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#performance-optimizations)  Performance Optimizations

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#memory-efficiency)  Memory Efficiency

Report incorrect code

Copy

Ask AI

```
# Stream processing prevents memory buildup for long texts
for i, (gs, ps, audio) in enumerate(generator):
    # Process incrementally rather than loading all at once
```

### [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#cost-optimization)  Cost Optimization

Report incorrect code

Copy

Ask AI

```
machine_type = "L"  # CPU is sufficient and cost-effective
keep_alive=3000     # Longer keep-alive reduces cold starts
```

## [‚Äã](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model\#key-takeaways)  Key Takeaways

- **CPU deployment** is ideal for lightweight models like Kokoro (82M parameters)
- **Multi-language support** requires separate pipelines and voice models
- **Character-based billing** aligns costs with resource usage
- **Audio streaming** handles long texts efficiently without memory issues
- **Temporary file handling** with CDN upload provides fast, reliable audio delivery
- **Multiple endpoints** with shared logic offer flexibility while maintaining DRY principles
- **Custom validation** provides better user experience with clear error messages

This pattern is perfect for building production-ready TTS services that need to support multiple languages and voices while maintaining cost efficiency and high performance through CPU-optimized deployment.

Was this page helpful?

YesNo

[Deploy a Text-to-Video Model\\
\\
Previous](https://docs.fal.ai/serverless/tutorials/deploy-text-to-video-model) [Deploy a Text-to-Music Model\\
\\
Next](https://docs.fal.ai/serverless/tutorials/deploy-text-to-music-model)

Ctrl+I

### üîí Need Serverless Access?

√ó

Don't have access to fal Serverless yet? Request access to deploy your custom models with instant GPU scaling.


[Request Access](https://fal.ai/dashboard/serverless-get-started)

Assistant

Responses are generated using AI and may contain mistakes.

[Create support ticket](mailto:support@fal.ai)