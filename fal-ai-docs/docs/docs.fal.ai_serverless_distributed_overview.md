---
url: "https://docs.fal.ai/serverless/distributed/overview"
title: "Overview - fal"
---

[Skip to main content](https://docs.fal.ai/serverless/distributed/overview#content-area)

[fal home page![light logo](https://mintcdn.com/fal-d8505a2e/_1QeqsRe91WUAOCJ/logo/light.svg?fit=max&auto=format&n=_1QeqsRe91WUAOCJ&q=85&s=04c374284984bf56c89974379f02b7a2)![dark logo](https://mintcdn.com/fal-d8505a2e/_1QeqsRe91WUAOCJ/logo/dark.svg?fit=max&auto=format&n=_1QeqsRe91WUAOCJ&q=85&s=b136c77964ac416a72cb0bcba775d7c7)](https://fal.ai/)

Search...

Ctrl KAsk AI

Search...

Navigation

Multi-GPU Workloads

Overview

[![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/home.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/home.svg)Home](https://docs.fal.ai/) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/ar-cube-1.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/ar-cube-1.svg)Model APIs](https://docs.fal.ai/model-apis) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/rocket.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/rocket.svg)Serverless](https://docs.fal.ai/serverless) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/chip.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/chip.svg)Compute](https://docs.fal.ai/compute) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/file-json.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/file-json.svg)Platform APIs](https://docs.fal.ai/platform-apis) [Changelog](https://docs.fal.ai/changelog)

- [Status](https://status.fal.ai/)
- [Community](https://discord.gg/fal-ai)
- [Blog](https://blog.fal.ai/)

- [Introduction](https://docs.fal.ai/serverless)

- [Connect to Cursor](https://docs.fal.ai/serverless/mcp)

##### Getting Started

- [Quick Start](https://docs.fal.ai/serverless/getting-started/quick-start)
- [Deploy Your First Image Generator](https://docs.fal.ai/serverless/getting-started/deploy-your-first-image-generator)
- [Installation & Setup](https://docs.fal.ai/serverless/getting-started/installation)
- [Core Concepts](https://docs.fal.ai/serverless/getting-started/core-concepts)

##### Tutorials

- [Deploy a Text-to-Image Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-image-model)
- [Deploy a Text-to-Video Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-video-model)
- [Deploy a Text-to-Speech Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model)
- [Deploy a Text-to-Music Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-music-model)
- [Deploy a ComfyUI SDXL Turbo App](https://docs.fal.ai/serverless/tutorials/deploy-comfyui-server)
- [Deploy Multi-GPU Inference](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-inference)
- [Deploy Models with Custom Containers](https://docs.fal.ai/serverless/tutorials/deploy-models-with-custom-containers)

##### Deployment & Operations

- [Deploy to Production](https://docs.fal.ai/serverless/deployment-operations/deploy-to-production)
- [Manage Deployments](https://docs.fal.ai/serverless/deployment-operations/manage-deployments)
- [Manage Secrets Securely](https://docs.fal.ai/serverless/deployment-operations/manage-secrets-securely)
- [Monitor Performance](https://docs.fal.ai/serverless/deployment-operations/monitor-performance)
- [Scale Your Application](https://docs.fal.ai/serverless/deployment-operations/scale-your-application)

##### Development

- [Handle Inputs and Outputs](https://docs.fal.ai/serverless/development/handle-inputs-and-outputs)
- [Download Model Weights and Files](https://docs.fal.ai/serverless/development/download-model-weights-and-files)
- [Import Code](https://docs.fal.ai/serverless/development/import-code)
- [Use Persistent Storage](https://docs.fal.ai/serverless/development/use-persistent-storage)
- [Streaming Endpoints](https://docs.fal.ai/serverless/development/streaming)
- [Realtime Endpoints](https://docs.fal.ai/serverless/development/realtime)
- [Test Models and Endpoints](https://docs.fal.ai/serverless/development/test-models-and-endpoints)
- [Use a Custom Container Image](https://docs.fal.ai/serverless/development/use-custom-container-image)
- [Handle request cancellations](https://docs.fal.ai/serverless/development/handle-cancellations)
- [Use KV Store](https://docs.fal.ai/serverless/development/use-kv-store)
- [Add Health Check Endpoint](https://docs.fal.ai/serverless/development/add-health-check-endpoint)

##### Multi-GPU Workloads

- [Overview](https://docs.fal.ai/serverless/distributed/overview)
- [Event Streaming](https://docs.fal.ai/serverless/distributed/streaming)
- [API Reference](https://docs.fal.ai/serverless/distributed/api-reference)

##### Advanced Optimizations

- [Optimize Routing Behavior](https://docs.fal.ai/serverless/optimizations/optimize-routing-behavior)
- [Optimize Model Performance](https://docs.fal.ai/serverless/optimizations/optimize-model-performance)
- [Optimize Startup with Compiled Caches](https://docs.fal.ai/serverless/optimizations/optimize-startup-with-compiled-caches)
- [Optimize Container Images](https://docs.fal.ai/serverless/optimizations/optimize-container-images)

##### Migrations

- [Migrate an External Docker Server](https://docs.fal.ai/serverless/migrations/migrate-external-docker-server)
- [Migrate from Replicate](https://docs.fal.ai/serverless/migrations/migrate-from-replicate)

##### CLI Reference

- [Installation](https://docs.fal.ai/serverless/cli/installation)
- [fal auth](https://docs.fal.ai/serverless/cli/auth)
- fal apps

- [fal deploy](https://docs.fal.ai/serverless/cli/deploy)
- [fal files](https://docs.fal.ai/serverless/cli/files)
- [fal run](https://docs.fal.ai/serverless/cli/run)
- [fal queue](https://docs.fal.ai/serverless/cli/queue)
- [fal keys](https://docs.fal.ai/serverless/cli/keys)
- [fal profile](https://docs.fal.ai/serverless/cli/profile)
- [fal secrets](https://docs.fal.ai/serverless/cli/secrets)
- [fal doctor](https://docs.fal.ai/serverless/cli/doctor)
- [fal create](https://docs.fal.ai/serverless/cli/create)
- [fal runners](https://docs.fal.ai/serverless/cli/runners)

##### Python SDK

- [fal.App Class Reference](https://docs.fal.ai/serverless/python/fal-app-reference)
- [fal.api.SyncServerlessClient](https://docs.fal.ai/serverless/python/client)
- [Python SDK API Reference](https://docs.fal.ai/serverless/python/api-reference)

##### API Reference

- [Platform APIs for Serverless](https://docs.fal.ai/serverless/platform-apis)

On this page

- [Why Use Multiple GPUs?](https://docs.fal.ai/serverless/distributed/overview#why-use-multiple-gpus)
- [For Inference](https://docs.fal.ai/serverless/distributed/overview#for-inference)
- [For Training](https://docs.fal.ai/serverless/distributed/overview#for-training)
- [Core Concepts](https://docs.fal.ai/serverless/distributed/overview#core-concepts)
- [Architecture Overview](https://docs.fal.ai/serverless/distributed/overview#architecture-overview)
- [DistributedRunner](https://docs.fal.ai/serverless/distributed/overview#distributedrunner)
- [Key Methods](https://docs.fal.ai/serverless/distributed/overview#key-methods)
- [Example Usage](https://docs.fal.ai/serverless/distributed/overview#example-usage)
- [DistributedWorker](https://docs.fal.ai/serverless/distributed/overview#distributedworker)
- [Methods to Override](https://docs.fal.ai/serverless/distributed/overview#methods-to-override)
- [Properties Youâ€™ll Use](https://docs.fal.ai/serverless/distributed/overview#properties-you%E2%80%99ll-use)
- [Utility Methods](https://docs.fal.ai/serverless/distributed/overview#utility-methods)
- [PyTorch Distributed Primitives](https://docs.fal.ai/serverless/distributed/overview#pytorch-distributed-primitives)
- [Parallelism Strategies](https://docs.fal.ai/serverless/distributed/overview#parallelism-strategies)
- [Inference Strategies](https://docs.fal.ai/serverless/distributed/overview#inference-strategies)
- [Training Strategies](https://docs.fal.ai/serverless/distributed/overview#training-strategies)
- [Configuration](https://docs.fal.ai/serverless/distributed/overview#configuration)
- [Specifying GPU Count](https://docs.fal.ai/serverless/distributed/overview#specifying-gpu-count)
- [Multi-GPU Machine Types](https://docs.fal.ai/serverless/distributed/overview#multi-gpu-machine-types)
- [Examples](https://docs.fal.ai/serverless/distributed/overview#examples)
- [Next Steps](https://docs.fal.ai/serverless/distributed/overview#next-steps)
- [Additional Resources](https://docs.fal.ai/serverless/distributed/overview#additional-resources)

The `fal.distributed` module enables you to scale your AI workloads across multiple GPUs, dramatically improving performance for both inference and training tasks. Whether you need to generate multiple images simultaneously or train large models faster, distributed computing on fal makes it straightforward.

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#why-use-multiple-gpus)  Why Use Multiple GPUs?

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#for-inference)  For Inference

- **Higher Throughput**: Generate multiple outputs simultaneously (e.g., 4 images at once on 4 GPUs)
- **Faster Single Output**: Split large models across GPUs for faster generation
- **Cost Efficiency**: Maximize GPU utilization for batch processing

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#for-training)  For Training

- **Faster Training**: Distribute training across multiple GPUs with synchronized gradient updates
- **Larger Batches**: Train with bigger batch sizes for better model convergence
- **Parallel Preprocessing**: Speed up data preprocessing by distributing it across GPUs

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#core-concepts)  Core Concepts

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#architecture-overview)  Architecture Overview

Hereâ€™s how the distributed computing components work together:

GPU N-1

GPU 1

GPU 0

fal.App

Your Application

â€¢ Defines num\_gpus

â€¢ Creates DistributedRunner

DistributedRunner

Orchestration Layer

â€¢ Spawns worker processes

â€¢ Manages communication (ZMQ)

â€¢ Coordinates execution

DistributedWorker

(Rank 0)

â€¢ self.device = cuda:0

â€¢ setup(): Load model

â€¢ **call**(): Process data

DistributedWorker

(Rank 1)

â€¢ self.device = cuda:1

â€¢ setup(): Load model

â€¢ **call**(): Process data

DistributedWorker

(Rank N-1)

â€¢ self.device = cuda:N-1

â€¢ setup(): Load model

â€¢ **call**(): Process data

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#distributedrunner)  DistributedRunner

The `DistributedRunner` is the main orchestration class that manages multi-GPU workloads. It handles process management, inter-process communication via ZMQ, and coordination between worker processes.

#### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#key-methods)  Key Methods

**`DistributedRunner(worker_cls, world_size)`**
Creates a runner that will spawn `world_size` instances of your `worker_cls`.**`await runner.start(**kwargs)`**
Launches all worker processes and calls their `setup()` method. Any `**kwargs` are passed to each workerâ€™s setup. Call this once in your appâ€™s `setup()`.**`await runner.invoke(payload)`**
Executes your workerâ€™s `__call__()` method with the given payload dict and returns the final result. Use this for standard (non-streaming) requests.**`runner.stream(payload, as_text_events=True)`**
Returns an async iterator that streams intermediate results from workers. Use this with `StreamingResponse` for real-time progress updates.

#### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#example-usage)  Example Usage

Report incorrect code

Copy

Ask AI

```
from fal.distributed import DistributedRunner, DistributedWorker

class MyWorker(DistributedWorker):
    def setup(self, model_path):
        # Load model on this GPU (called once per worker)
        self.model = load_model(model_path).to(self.device)

    def __call__(self, prompt, **kwargs):
        # Process request (called for each request)
        return self.model.generate(prompt)

class MyApp(fal.App):
    num_gpus = 4  # Use 4 GPUs

    async def setup(self):
        # Create and start the runner
        self.runner = DistributedRunner(
            worker_cls=MyWorker,
            world_size=self.num_gpus
        )
        await self.runner.start(model_path="/data/model")

    @fal.endpoint("/")
    async def run(self, request: MyRequest):
        # Invoke workers for each request
        result = await self.runner.invoke({
            "prompt": request.prompt,
        })
        return result
```

For complete API documentation with all parameters and examples, see the [DistributedRunner API Reference](https://docs.fal.ai/serverless/distributed/api-reference#distributedrunner).

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#distributedworker)  DistributedWorker

Your custom worker class extends `DistributedWorker` and runs on each GPU. Each worker is independent and has its own copy of the model.

#### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#methods-to-override)  Methods to Override

**`def setup(self, **kwargs)`**
Called once when the worker initializes. Load your model, download weights, and prepare resources here. Receives any `**kwargs` passed to `runner.start()`.**`def __call__(self, streaming=False, **kwargs)`**
Called for each request. Implement your inference or training logic here. Receives the payload dict from `runner.invoke()` or `runner.stream()` as `**kwargs`.

#### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#properties-you%E2%80%99ll-use)  Properties Youâ€™ll Use

**`self.device`** \- The PyTorch CUDA device for this worker (`cuda:0`, `cuda:1`, etc.)**`self.rank`** \- The workerâ€™s rank (0 to world\_size-1). Rank 0 is typically responsible for returning results.**`self.world_size`** \- Total number of workers (GPUs).

#### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#utility-methods)  Utility Methods

**`self.add_streaming_result(data, as_text_event=True)`**
Sends intermediate results to the client during streaming. Only call from rank 0 to avoid duplicates.**`self.rank_print(message)`**
Prints a message with the workerâ€™s rank prefix for easier debugging.

For complete API documentation with detailed examples, see the [DistributedWorker API Reference](https://docs.fal.ai/serverless/distributed/api-reference#distributedworker).

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#pytorch-distributed-primitives)  PyTorch Distributed Primitives

For training and coordinated inference, you can use PyTorchâ€™s distributed primitives:

Report incorrect code

Copy

Ask AI

```
import torch.distributed as dist

# Gather results from all GPUs to rank 0
dist.gather(tensor, gather_list if self.rank == 0 else None, dst=0)

# Broadcast data from rank 0 to all GPUs
dist.broadcast(tensor, src=0)

# Synchronize all GPUs at a barrier
dist.barrier()
```

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#parallelism-strategies)  Parallelism Strategies

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#inference-strategies)  Inference Strategies

Different parallelism strategies optimize for different inference scenarios. Choose based on your use case and model architecture.

## Data Parallelism

Each GPU runs an independent model copy with different inputs. Best for high throughput scenarios where you need to generate multiple outputs simultaneously.**Use for:** Batch processing, generating multiple image variations, high throughput workloads

**Example:** Parallel SDXL in fal-demos

## Pipeline Parallelism (PipeFusion)

Split the model into sequential stages across GPUs, processing like an assembly line where each GPU handles specific layers. Reduces latency for single outputs.**Use for:** Large DiT models (SD3, FLUX), reducing latency for single image generation

**Example:** xFuserâ€™s PipeFusion implementation for DiT models

## Tensor Parallelism

Split individual layers and tensors across GPUs, computing portions of each layer in parallel. Required when models are too large to fit on a single GPU.**Use for:** Extremely large models that donâ€™t fit on single GPU memory

**Example:** Large language models, very large diffusion models

## Sequence Parallelism (Ulysses)

Split attention computation across the sequence or spatial dimensions. Particularly effective for long sequences and can be combined with other strategies.**Use for:** Very long sequences, high-resolution images, combining with PipeFusion

**Example:** xFuserâ€™s Ulysses implementation

## CFG Parallelism

Parallel conditional and unconditional passes for classifier-free guidance. Runs both guidance passes simultaneously on separate GPUs.**Use for:** U-Net models (SDXL), requires exactly 2 GPUs

**Example:** xFuser supports this for U-Net architectures

## Hybrid Strategies

Combine multiple approaches (e.g., PipeFusion + Ulysses + CFG) for maximum scaling efficiency across many GPUs.**Use for:** Maximum scaling across 4-8+ GPUs, complex production workloads

**Example:** xFuser configurations for 8 GPU setups

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#training-strategies)  Training Strategies

Multi-GPU training strategies focus on distributing the computational and memory requirements of training large models.

## Distributed Data Parallel (DDP)

Each GPU has a full model copy and processes different data batches. Gradients are synchronized across all GPUs after each backward pass, ensuring all models stay identical.**Use for:** Standard multi-GPU training, best scaling for most use cases

**Example:** Flux LoRA training in fal-demos

## Pipeline Parallelism

Split model into stages and process microbatches through the pipeline. Requires careful load balancing to avoid GPU idle time (bubble overhead).**Use for:** Very large models, when combined with other parallelism strategies

**Example:** GPT-3 style training

## Tensor Parallelism

Split model layers across GPUs using Megatron-LM style parallelization. Each layerâ€™s computation is distributed across multiple GPUs.**Use for:** Models too large for single GPU even with gradient checkpointing

**Example:** Large transformer training

## FSDP/ZeRO

Fully Sharded Data Parallel (FSDP) or ZeRO optimizer shard optimizer states, gradients, and parameters across GPUs to reduce memory footprint per GPU.**Use for:** Training very large models with memory constraints, scaling beyond DDP

**Example:** Large model training (70B+ parameters)

Many production workloads benefit from hybrid strategies. For example, xFuser can combine PipeFusion + Ulysses + CFG parallelism to scale across 8+ GPUs efficiently.

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#configuration)  Configuration

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#specifying-gpu-count)  Specifying GPU Count

Report incorrect code

Copy

Ask AI

```
class MyApp(fal.App):
    num_gpus = 8  # Request 8 GPUs
    machine_type = "GPU-H100"  # Each GPU will be an H100
```

### [â€‹](https://docs.fal.ai/serverless/distributed/overview\#multi-gpu-machine-types)  Multi-GPU Machine Types

fal supports various multi-GPU configurations:

- `GPU-H100` with `num_gpus=2`: 2x H100 GPUs
- `GPU-H100` with `num_gpus=4`: 4x H100 GPUs
- `GPU-H100` with `num_gpus=8`: 8x H100 GPUs
- `GPU-A100` with `num_gpus=2/4/8`: 2-8x A100 GPUs

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#examples)  Examples

All examples are available in the [fal-demos repository](https://github.com/fal-ai-community/fal-demos/tree/main/fal_demos/distributed):

- **Parallel SDXL**: Data parallelism for image generation ( [code](https://github.com/fal-ai-community/fal-demos/blob/main/fal_demos/distributed/inference/parallel_sdxl/app.py))
- **xFuser**: Model parallelism with DiT models ( [code](https://github.com/fal-ai-community/fal-demos/blob/main/fal_demos/distributed/inference/xfuser/app.py))
- **Flux LoRA Training**: Complete DDP training pipeline ( [code](https://github.com/fal-ai-community/fal-demos/tree/main/fal_demos/distributed/training/flux_lora))

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#next-steps)  Next Steps

[**API Reference** \\
\\
Complete API documentation for DistributedRunner and DistributedWorker](https://docs.fal.ai/serverless/distributed/api-reference) [**Multi-GPU Inference Tutorial** \\
\\
Step-by-step guide with parallel SDXL](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-inference) [**Multi-GPU Training Tutorial** \\
\\
Build a distributed Flux LoRA training service](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-training) [**Event Streaming** \\
\\
Stream intermediate results from workers](https://docs.fal.ai/serverless/distributed/streaming)

## [â€‹](https://docs.fal.ai/serverless/distributed/overview\#additional-resources)  Additional Resources

- [fal-demos distributed examples](https://github.com/fal-ai-community/fal-demos/tree/main/fal_demos/distributed) \- Complete working code
- [Python SDK Reference](https://docs.fal.ai/serverless/python/api-reference) \- Full Python API
- [Core Concepts](https://docs.fal.ai/serverless/getting-started/core-concepts) \- Fundamental fal concepts

Was this page helpful?

YesNo

[Add Health Check Endpoint\\
\\
Previous](https://docs.fal.ai/serverless/development/add-health-check-endpoint) [Event Streaming\\
\\
Next](https://docs.fal.ai/serverless/distributed/streaming)

Ctrl+I

### ðŸ”’ Need Serverless Access?

Ã—

Don't have access to fal Serverless yet? Request access to deploy your custom models with instant GPU scaling.


[Request Access](https://fal.ai/dashboard/serverless-get-started)

Assistant

Responses are generated using AI and may contain mistakes.

[Create support ticket](mailto:support@fal.ai)