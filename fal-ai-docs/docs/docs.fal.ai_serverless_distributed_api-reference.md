---
url: "https://docs.fal.ai/serverless/distributed/api-reference"
title: "API Reference - fal"
---

[Skip to main content](https://docs.fal.ai/serverless/distributed/api-reference#content-area)

[fal home page![light logo](https://mintcdn.com/fal-d8505a2e/_1QeqsRe91WUAOCJ/logo/light.svg?fit=max&auto=format&n=_1QeqsRe91WUAOCJ&q=85&s=04c374284984bf56c89974379f02b7a2)![dark logo](https://mintcdn.com/fal-d8505a2e/_1QeqsRe91WUAOCJ/logo/dark.svg?fit=max&auto=format&n=_1QeqsRe91WUAOCJ&q=85&s=b136c77964ac416a72cb0bcba775d7c7)](https://fal.ai/)

Search...

Ctrl KAsk AI

Search...

Navigation

Multi-GPU Workloads

API Reference

[![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/home.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/home.svg)Home](https://docs.fal.ai/) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/ar-cube-1.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/ar-cube-1.svg)Model APIs](https://docs.fal.ai/model-apis) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/rocket.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/rocket.svg)Serverless](https://docs.fal.ai/serverless) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/chip.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/chip.svg)Compute](https://docs.fal.ai/compute) [![https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/file-json.svg](https://mintlify.s3.us-west-1.amazonaws.com/fal-d8505a2e/images/icons/file-json.svg)Platform APIs](https://docs.fal.ai/platform-apis) [Changelog](https://docs.fal.ai/changelog)

- [Status](https://status.fal.ai/)
- [Community](https://discord.gg/fal-ai)
- [Blog](https://blog.fal.ai/)

- [Introduction](https://docs.fal.ai/serverless)

- [Connect to Cursor](https://docs.fal.ai/serverless/mcp)

##### Getting Started

- [Quick Start](https://docs.fal.ai/serverless/getting-started/quick-start)
- [Deploy Your First Image Generator](https://docs.fal.ai/serverless/getting-started/deploy-your-first-image-generator)
- [Installation & Setup](https://docs.fal.ai/serverless/getting-started/installation)
- [Core Concepts](https://docs.fal.ai/serverless/getting-started/core-concepts)

##### Tutorials

- [Deploy a Text-to-Image Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-image-model)
- [Deploy a Text-to-Video Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-video-model)
- [Deploy a Text-to-Speech Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-speech-model)
- [Deploy a Text-to-Music Model](https://docs.fal.ai/serverless/tutorials/deploy-text-to-music-model)
- [Deploy a ComfyUI SDXL Turbo App](https://docs.fal.ai/serverless/tutorials/deploy-comfyui-server)
- [Deploy Multi-GPU Inference](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-inference)
- [Deploy Models with Custom Containers](https://docs.fal.ai/serverless/tutorials/deploy-models-with-custom-containers)

##### Deployment & Operations

- [Deploy to Production](https://docs.fal.ai/serverless/deployment-operations/deploy-to-production)
- [Manage Deployments](https://docs.fal.ai/serverless/deployment-operations/manage-deployments)
- [Manage Secrets Securely](https://docs.fal.ai/serverless/deployment-operations/manage-secrets-securely)
- [Monitor Performance](https://docs.fal.ai/serverless/deployment-operations/monitor-performance)
- [Scale Your Application](https://docs.fal.ai/serverless/deployment-operations/scale-your-application)

##### Development

- [Handle Inputs and Outputs](https://docs.fal.ai/serverless/development/handle-inputs-and-outputs)
- [Download Model Weights and Files](https://docs.fal.ai/serverless/development/download-model-weights-and-files)
- [Import Code](https://docs.fal.ai/serverless/development/import-code)
- [Use Persistent Storage](https://docs.fal.ai/serverless/development/use-persistent-storage)
- [Streaming Endpoints](https://docs.fal.ai/serverless/development/streaming)
- [Realtime Endpoints](https://docs.fal.ai/serverless/development/realtime)
- [Test Models and Endpoints](https://docs.fal.ai/serverless/development/test-models-and-endpoints)
- [Use a Custom Container Image](https://docs.fal.ai/serverless/development/use-custom-container-image)
- [Handle request cancellations](https://docs.fal.ai/serverless/development/handle-cancellations)
- [Use KV Store](https://docs.fal.ai/serverless/development/use-kv-store)
- [Add Health Check Endpoint](https://docs.fal.ai/serverless/development/add-health-check-endpoint)

##### Multi-GPU Workloads

- [Overview](https://docs.fal.ai/serverless/distributed/overview)
- [Event Streaming](https://docs.fal.ai/serverless/distributed/streaming)
- [API Reference](https://docs.fal.ai/serverless/distributed/api-reference)

##### Advanced Optimizations

- [Optimize Routing Behavior](https://docs.fal.ai/serverless/optimizations/optimize-routing-behavior)
- [Optimize Model Performance](https://docs.fal.ai/serverless/optimizations/optimize-model-performance)
- [Optimize Startup with Compiled Caches](https://docs.fal.ai/serverless/optimizations/optimize-startup-with-compiled-caches)
- [Optimize Container Images](https://docs.fal.ai/serverless/optimizations/optimize-container-images)

##### Migrations

- [Migrate an External Docker Server](https://docs.fal.ai/serverless/migrations/migrate-external-docker-server)
- [Migrate from Replicate](https://docs.fal.ai/serverless/migrations/migrate-from-replicate)

##### CLI Reference

- [Installation](https://docs.fal.ai/serverless/cli/installation)
- [fal auth](https://docs.fal.ai/serverless/cli/auth)
- fal apps

- [fal deploy](https://docs.fal.ai/serverless/cli/deploy)
- [fal files](https://docs.fal.ai/serverless/cli/files)
- [fal run](https://docs.fal.ai/serverless/cli/run)
- [fal queue](https://docs.fal.ai/serverless/cli/queue)
- [fal keys](https://docs.fal.ai/serverless/cli/keys)
- [fal profile](https://docs.fal.ai/serverless/cli/profile)
- [fal secrets](https://docs.fal.ai/serverless/cli/secrets)
- [fal doctor](https://docs.fal.ai/serverless/cli/doctor)
- [fal create](https://docs.fal.ai/serverless/cli/create)
- [fal runners](https://docs.fal.ai/serverless/cli/runners)

##### Python SDK

- [fal.App Class Reference](https://docs.fal.ai/serverless/python/fal-app-reference)
- [fal.api.SyncServerlessClient](https://docs.fal.ai/serverless/python/client)
- [Python SDK API Reference](https://docs.fal.ai/serverless/python/api-reference)

##### API Reference

- [Platform APIs for Serverless](https://docs.fal.ai/serverless/platform-apis)

On this page

- [DistributedRunner](https://docs.fal.ai/serverless/distributed/api-reference#distributedrunner)
- [Constructor](https://docs.fal.ai/serverless/distributed/api-reference#constructor)
- [start()](https://docs.fal.ai/serverless/distributed/api-reference#start)
- [invoke()](https://docs.fal.ai/serverless/distributed/api-reference#invoke)
- [stream()](https://docs.fal.ai/serverless/distributed/api-reference#stream)
- [DistributedWorker](https://docs.fal.ai/serverless/distributed/api-reference#distributedworker)
- [Properties](https://docs.fal.ai/serverless/distributed/api-reference#properties)
- [device](https://docs.fal.ai/serverless/distributed/api-reference#device)
- [rank](https://docs.fal.ai/serverless/distributed/api-reference#rank)
- [world\_size](https://docs.fal.ai/serverless/distributed/api-reference#world-size)
- [Methods to Override](https://docs.fal.ai/serverless/distributed/api-reference#methods-to-override)
- [setup()](https://docs.fal.ai/serverless/distributed/api-reference#setup)
- [call()](https://docs.fal.ai/serverless/distributed/api-reference#call)
- [Utility Methods](https://docs.fal.ai/serverless/distributed/api-reference#utility-methods)
- [add\_streaming\_result()](https://docs.fal.ai/serverless/distributed/api-reference#add-streaming-result)
- [rank\_print()](https://docs.fal.ai/serverless/distributed/api-reference#rank-print)
- [Common Patterns](https://docs.fal.ai/serverless/distributed/api-reference#common-patterns)
- [Pattern 1: Data Parallelism (Inference)](https://docs.fal.ai/serverless/distributed/api-reference#pattern-1%3A-data-parallelism-inference)
- [Pattern 2: Distributed Data Parallel (Training)](https://docs.fal.ai/serverless/distributed/api-reference#pattern-2%3A-distributed-data-parallel-training)
- [Pattern 3: Streaming with Progress Updates](https://docs.fal.ai/serverless/distributed/api-reference#pattern-3%3A-streaming-with-progress-updates)
- [Next Steps](https://docs.fal.ai/serverless/distributed/api-reference#next-steps)

This page covers the essential APIs for building multi-GPU applications with `fal.distributed`. We focus on the methods youâ€™ll actually use in your code.

## [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#distributedrunner)  DistributedRunner

The `DistributedRunner` class orchestrates multiple GPU workers for distributed computation. It handles process management, inter-process communication via ZMQ, and coordination between worker processes.

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#constructor)  Constructor

Report incorrect code

Copy

Ask AI

```
DistributedRunner(
    worker_cls: type[DistributedWorker],
    world_size: int,
)
```

**Parameters:**

- **`worker_cls`** (`type[DistributedWorker]`): Your custom worker class that inherits from `DistributedWorker`.
- **`world_size`** (`int`): Total number of worker processes to spawn (typically equals `num_gpus`).

**Example:**

Report incorrect code

Copy

Ask AI

```
from fal.distributed import DistributedRunner, DistributedWorker

class MyWorker(DistributedWorker):
    def setup(self, **kwargs):
        self.model = load_model().to(self.device)

    def __call__(self, prompt: str, **kwargs):
        return self.model.generate(prompt)

# Create runner for 4 GPUs
runner = DistributedRunner(
    worker_cls=MyWorker,
    world_size=4,
)
```

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#start)  start()

Starts all distributed worker processes and initializes them.

Report incorrect code

Copy

Ask AI

```
async def start(
    self,
    timeout: int = 1800,
    **kwargs: Any
) -> None
```

**Parameters:**

- **`timeout`** (`int`): Maximum time (in seconds) to wait for all workers to be ready. Default: `1800` (30 minutes).
- **`**kwargs`**: Additional keyword arguments passed to each workerâ€™s `setup()` method.

**Raises:**

- `RuntimeError`: If processes are already running or fail to start.
- `TimeoutError`: If workers donâ€™t become ready within the timeout period.

**Example:**

Report incorrect code

Copy

Ask AI

```
class MyApp(fal.App):
    num_gpus = 2

    async def setup(self):
        self.runner = DistributedRunner(
            worker_cls=MyWorker,
            world_size=self.num_gpus,
        )

        # Start workers and pass model path to their setup()
        await self.runner.start(model_path="/data/models/flux")

        # Workers are now ready to process requests
```

**What it does:**

1. Spawns `world_size` worker processes (one per GPU)
2. Each worker runs its `setup()` method with the provided `**kwargs`
3. Waits for all workers to signal â€œREADYâ€
4. Starts the keepalive timer if configured
5. Returns when all workers are initialized and ready

This method must be called before using `invoke()` or `stream()`. Itâ€™s typically called once in your appâ€™s `setup()` method.

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#invoke)  invoke()

Executes the workerâ€™s `__call__()` method across all GPUs and returns the final result from rank 0.

Report incorrect code

Copy

Ask AI

```
async def invoke(
    self,
    payload: dict[str, Any] = {},
    timeout: int | None = None,
) -> Any
```

**Parameters:**

- **`payload`** (`dict[str, Any]`): Dictionary of arguments to pass to each workerâ€™s `__call__()` method. Default: `{}`.
- **`timeout`** (`int | None`): Maximum time (in seconds) to wait for the result. If `None`, uses the runnerâ€™s default timeout. Default: `None`.

**Returns:**

- `Any`: The result returned by rank 0 workerâ€™s `__call__()` method.

**Raises:**

- `RuntimeError`: If workers are not running or encounter an error during execution.
- `TimeoutError`: If the operation exceeds the timeout.

**Example:**

Report incorrect code

Copy

Ask AI

```
@fal.endpoint("/generate")
async def generate(self, request: GenerateRequest) -> GenerateResponse:
    # Invoke workers to generate images
    result = await self.runner.invoke({
        "prompt": request.prompt,
        "num_steps": request.num_steps,
        "width": 1024,
        "height": 1024,
    })

    return GenerateResponse(image=result["image"])
```

**How it works:**

1. Serializes the payload and sends it to all workers
2. Each worker executes its `__call__()` method with `streaming=False`
3. Workers coordinate using PyTorch distributed operations (e.g., `dist.gather()`)
4. Only rank 0 returns the result
5. Result is deserialized and returned to the caller

Use `invoke()` for standard (non-streaming) requests where you need the final result only.

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#stream)  stream()

Streams intermediate results from workers during execution, useful for long-running operations like image generation or training.

Report incorrect code

Copy

Ask AI

```
async def stream(
    self,
    payload: dict[str, Any] = {},
    timeout: int | None = None,
    streaming_timeout: int | None = None,
    as_text_events: bool = False,
) -> AsyncIterator[Any]
```

**Parameters:**

- **`payload`** (`dict[str, Any]`): Dictionary of arguments to pass to each workerâ€™s `__call__()` method. Default: `{}`.
- **`timeout`** (`int | None`): Maximum total time (in seconds) for the entire operation. Default: `None` (no limit).
- **`streaming_timeout`** (`int | None`): Maximum time (in seconds) between consecutive yields. If no data is received within this period, raises `TimeoutError`. Default: `None`.
- **`as_text_events`** (`bool`): If `True`, yields Server-Sent Events (SSE) formatted as bytes. If `False`, yields deserialized Python objects. Default: `False`.

**Returns:**

- `AsyncIterator[Any]`: Async iterator yielding intermediate results and the final result.

**Raises:**

- `RuntimeError`: If workers are not running, encounter an error, or yield no data.
- `TimeoutError`: If the operation exceeds timeout or streaming\_timeout.

**Example:**

Report incorrect code

Copy

Ask AI

```
@fal.endpoint("/stream")
async def stream_generate(self, request: GenerateRequest) -> StreamingResponse:
    return StreamingResponse(
        self.runner.stream(
            payload={
                "prompt": request.prompt,
                "num_steps": request.num_steps,
            },
            as_text_events=True,
        ),
        media_type="text/event-stream",
    )
```

**How it works:**

1. Serializes the payload and sends it to all workers
2. Each worker executes its `__call__()` method with `streaming=True`
3. Workers can call `self.add_streaming_result()` to send intermediate updates
4. The runner yields each intermediate result as itâ€™s received
5. After workers finish, yields the final result
6. Automatically handles serialization based on `as_text_events`

Set `as_text_events=True` when using with `StreamingResponse` for browser-compatible Server-Sent Events.

* * *

## [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#distributedworker)  DistributedWorker

The `DistributedWorker` class is the base class for your custom GPU workers. Each instance runs on a separate GPU and handles model loading, inference, or training.Create your own worker by inheriting from `DistributedWorker` and overriding the `setup()` and `__call__()` methods.

Report incorrect code

Copy

Ask AI

```
class MyWorker(DistributedWorker):
    def setup(self, **kwargs):
        # Load model on this GPU
        self.model = load_model().to(self.device)

    def __call__(self, prompt: str, **kwargs):
        # Process request
        return self.model.generate(prompt)
```

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#properties)  Properties

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#device)  device

Returns the CUDA device assigned to this worker.

Report incorrect code

Copy

Ask AI

```
@property
def device(self) -> torch.device
```

**Returns:**

- `torch.device`: The PyTorch device for this worker, e.g., `cuda:0`, `cuda:1`, etc.

**Example:**

Report incorrect code

Copy

Ask AI

```
class MyWorker(DistributedWorker):
    def setup(self):
        # Load model on this worker's GPU
        self.model = MyModel().to(self.device)
        print(f"Model loaded on {self.device}")
```

* * *

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#rank)  rank

The rank (ID) of this worker, from 0 to world\_size-1.

Report incorrect code

Copy

Ask AI

```
self.rank: int
```

**Example:**

Report incorrect code

Copy

Ask AI

```
if self.rank == 0:
    print("I'm the master worker!")
    # Only rank 0 saves checkpoints, uploads files, etc.
```

* * *

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#world-size)  world\_size

Total number of workers in the distributed setup.

Report incorrect code

Copy

Ask AI

```
self.world_size: int
```

**Example:**

Report incorrect code

Copy

Ask AI

```
print(f"Running with {self.world_size} GPUs")
```

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#methods-to-override)  Methods to Override

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#setup)  setup()

Called once when the worker is initialized. Use this to load models, download weights, and prepare resources.

Report incorrect code

Copy

Ask AI

```
def setup(self, **kwargs: Any) -> None
```

**Parameters:**

- **`**kwargs`**: Any keyword arguments passed to `runner.start()`.

**Example:**

Report incorrect code

Copy

Ask AI

```
class FluxWorker(DistributedWorker):
    def setup(self, model_path: str = "/data/flux", **kwargs):
        """Initialize the Flux model on this GPU"""
        import torch
        from diffusers import FluxPipeline

        self.rank_print(f"Loading Flux on {self.device}")

        self.pipeline = FluxPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
        ).to(self.device)

        # Disable progress bar for non-main workers
        if self.rank != 0:
            self.pipeline.set_progress_bar_config(disable=True)

        self.rank_print("Model loaded successfully")
```

Heavy operations like model loading should go in `setup()`, not `__call__()`, so they only happen once per worker.

* * *

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#call)  **call**()

Called for each request. Implement your main processing logic here.

Report incorrect code

Copy

Ask AI

```
def __call__(self, streaming: bool = False, **kwargs: Any) -> Any
```

**Parameters:**

- **`streaming`** (`bool`): `True` if called via `runner.stream()`, `False` if called via `runner.invoke()`.
- **`**kwargs`**: Arguments from the `payload` dict passed to `runner.invoke()` or `runner.stream()`.

**Returns:**

- `Any`: The result to return. Only rank 0â€™s return value is sent back to the caller.

**Example:**

Report incorrect code

Copy

Ask AI

```
class FluxWorker(DistributedWorker):
    def __call__(
        self,
        prompt: str,
        num_steps: int = 20,
        streaming: bool = False,
        **kwargs
    ) -> dict:
        """Generate an image on this GPU"""
        import torch.distributed as dist

        # Each GPU generates independently
        image = self.pipeline(
            prompt=prompt,
            num_inference_steps=num_steps,
            output_type="pt",
        ).images[0]

        # Gather all images to rank 0
        if self.rank == 0:
            gather_list = [\
                torch.zeros_like(image, device=self.device)\
                for _ in range(self.world_size)\
            ]
        else:
            gather_list = None

        dist.gather(image, gather_list, dst=0)

        # Only rank 0 returns the result
        if self.rank == 0:
            combined_image = create_grid(gather_list)
            return {"image": combined_image}

        return {}  # Other ranks return empty dict
```

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#utility-methods)  Utility Methods

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#add-streaming-result)  add\_streaming\_result()

Sends an intermediate result to the client during streaming.

Report incorrect code

Copy

Ask AI

```
def add_streaming_result(
    self,
    result: Any,
    image_format: str = "jpeg",
    as_text_event: bool = False,
) -> None
```

**Parameters:**

- **`result`** (`Any`): The data to stream. Can be a dict, PIL image, or any serializable object.
- **`image_format`** (`str`): Image format for PIL images (`"jpeg"` or `"png"`). Default: `"jpeg"`.
- **`as_text_event`** (`bool`): If `True`, formats as Server-Sent Event. Must match the `as_text_events` parameter in `runner.stream()`. Default: `False`.

**Example:**

Report incorrect code

Copy

Ask AI

```
def __call__(self, prompt: str, num_steps: int = 20, streaming: bool = False):
    for step in range(num_steps):
        # Generate intermediate result
        latent = self.model.step(prompt)

        # Stream progress (only rank 0)
        if streaming and self.rank == 0 and step % 5 == 0:
            preview_image = self.decode_latent(latent)
            self.add_streaming_result({
                "step": step,
                "progress": (step + 1) / num_steps,
                "preview": preview_image,
            }, as_text_event=True)

    # Return final result
    return {"image": final_image}
```

Only call `add_streaming_result()` from rank 0 to avoid duplicate messages to the client.

* * *

#### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#rank-print)  rank\_print()

Prints a message with the workerâ€™s rank prefix for easy debugging.

Report incorrect code

Copy

Ask AI

```
def rank_print(self, message: str, debug: bool = False) -> None
```

**Parameters:**

- **`message`** (`str`): The message to print.
- **`debug`** (`bool`): If `True`, prefixes with `[debug]`. Default: `False`.

**Example:**

Report incorrect code

Copy

Ask AI

```
self.rank_print("Starting generation...")
# Output: [rank 0] Starting generation...

self.rank_print("Model loaded", debug=True)
# Output: [debug] [rank 0] Model loaded
```

* * *

## [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#common-patterns)  Common Patterns

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#pattern-1:-data-parallelism-inference)  Pattern 1: Data Parallelism (Inference)

Each GPU processes different data independently:

Report incorrect code

Copy

Ask AI

```
class ParallelWorker(DistributedWorker):
    def __call__(self, prompt: str, **kwargs):
        import torch.distributed as dist

        # Each GPU generates independently with different seed
        result = self.model.generate(prompt)

        # Gather all results to rank 0
        if self.rank == 0:
            gather_list = [torch.zeros_like(result) for _ in range(self.world_size)]
        else:
            gather_list = None

        dist.gather(result, gather_list, dst=0)

        # Only rank 0 returns combined result
        if self.rank == 0:
            return {"outputs": gather_list}
        return {}
```

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#pattern-2:-distributed-data-parallel-training)  Pattern 2: Distributed Data Parallel (Training)

All GPUs have the same model, process different batches, and sync gradients:

Report incorrect code

Copy

Ask AI

```
class DDPWorker(DistributedWorker):
    def setup(self, **kwargs):
        from torch.nn.parallel import DistributedDataParallel as DDP

        self.model = MyModel().to(self.device)

        # Wrap with DDP for gradient synchronization
        self.model = DDP(
            self.model,
            device_ids=[self.rank],
            output_device=self.rank,
        )

        self.optimizer = torch.optim.Adam(self.model.parameters())

    def __call__(self, data_path: str, **kwargs):
        import torch.distributed as dist

        # Load and distribute data
        if self.rank == 0:
            data = load_data(data_path)
        else:
            data = None

        # Broadcast to all ranks
        data = dist.broadcast_object_list([data], src=0)[0]

        # Each GPU processes different batch
        local_batch = data[self.rank::self.world_size]

        # Training loop
        for batch in local_batch:
            loss = self.model(batch)
            loss.backward()  # DDP syncs gradients automatically
            self.optimizer.step()
            self.optimizer.zero_grad()

        # Only rank 0 saves checkpoint
        if self.rank == 0:
            torch.save(self.model.state_dict(), "checkpoint.pt")
            return {"checkpoint": "checkpoint.pt"}

        return {}
```

* * *

### [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#pattern-3:-streaming-with-progress-updates)  Pattern 3: Streaming with Progress Updates

Stream intermediate results during long-running operations:

Report incorrect code

Copy

Ask AI

```
class StreamingWorker(DistributedWorker):
    def __call__(self, prompt: str, steps: int = 50, streaming: bool = False):
        import torch.distributed as dist

        for step in range(steps):
            result = self.model.step(prompt)

            # Stream progress every 5 steps
            if streaming and self.rank == 0 and step % 5 == 0:
                self.add_streaming_result({
                    "step": step,
                    "progress": step / steps,
                }, as_text_event=True)

            # Sync all workers
            dist.barrier()

        # Return final result
        if self.rank == 0:
            return {"output": result}
        return {}
```

* * *

## [â€‹](https://docs.fal.ai/serverless/distributed/api-reference\#next-steps)  Next Steps

[**Multi-GPU Inference Tutorial** \\
\\
Complete example with data parallelism](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-inference) [**Multi-GPU Training Tutorial** \\
\\
Complete example with DDP training](https://docs.fal.ai/serverless/tutorials/deploy-multi-gpu-training) [**Event Streaming** \\
\\
Learn about streaming intermediate results](https://docs.fal.ai/serverless/distributed/streaming) [**Overview** \\
\\
High-level overview of multi-GPU workloads](https://docs.fal.ai/serverless/distributed/overview)

Was this page helpful?

YesNo

[Event Streaming\\
\\
Previous](https://docs.fal.ai/serverless/distributed/streaming) [Optimize Routing Behavior\\
\\
Next](https://docs.fal.ai/serverless/optimizations/optimize-routing-behavior)

Ctrl+I

### ðŸ”’ Need Serverless Access?

Ã—

Don't have access to fal Serverless yet? Request access to deploy your custom models with instant GPU scaling.


[Request Access](https://fal.ai/dashboard/serverless-get-started)

Assistant

Responses are generated using AI and may contain mistakes.

[Create support ticket](mailto:support@fal.ai)